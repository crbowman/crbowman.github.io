---
layout: post
title: Term Frequency-Inverse Document Frequency
modified: 2018-5-2
tags: [algorithms, nlp]
image:
  feature: generative-5.png
---
## Term Frequency-Inverse Document Frequency

### What is TF-IDF?
TF-IDF stands for *term frequency-inverse document frequency* and is a measurement used as a term weight for a corpus of documents with a vector space model representation (typically a *bag of words model*).
This weight is used to indicate the importance of a particular word in a document relative to the corpus as a whole. The weight of a word increases proportionally to the number of apperances in a document, 
but is offset by decreasing in proportion the number of appearances in the corpus as a whole. TF-IDF is an increadibly popular term-weighting scheme, and is useful for a wide variety of information retireval tasks such as document classification, sentiment analysis and named entity recognition.

TF-IDF is the product of two statistics, term frequency and inverse document frequency. There are a variety of ways to calculate the value of each of these statistics.

### Term Frequency

For **term frequency** $$\text{tf}(t,d)$$, the most straightforward calculation is to use a raw count of a term in a document. This raw count is deonted  $$\, f_{\textit{t,d}}$$, i.e. the frequency $$\, f$$ that term *t* occurs in document *d*. This calculation of term frequency works for many applications, but it is often necessary to take into account the length of the document. A term that appears three times in a 500 word document is more signifigant than a term that appears three times in a 5,000 word document. To make this adjustment we can divide $$\, f_{\textit{t,d}}$$ by the total number of words in the document. 

$$\text{tf}(\textit{t,d}) = \frac{f_{\textit{t,d}}}{\sum\limits_{t^\prime \in d}^{} f_{t^\prime,d}}$$

In some scenarios the importance of a term may not scale proportionally with term frequency. We may want to give the first appearance of a term more importance than the 100th appearance. Using a sublinear function can help with this adjustment. Logorithmic normalization is the most common way to achieve this.

$$
\text{tf}(\textit{t,d}) =  \begin{cases}
		      \log(1 + f_{\textit{t,d}}), & \text{if} \; f_{\textit{t,d}} > 0\\
		      0, & \text{otherwise}
		    \end{cases}
$$

### Inverse Document Frequency

Term frequency alone suffers a crucial problem: each term is considered equally important when we're analyzing our entire corpus of documents. We would expect that some terms would have little or no signifigance to the nlp task we wish to perform on our corpus. For instance, a collection of history articles is likely to have the term history in every document. Towards this end, we introduce a mechanism that minimizes the affect of terms that occur too frequently in the corpus to be meaningful. Document frequency $$\text{df}(t,D)$$ is defined as the number of documents in a corpus $$D$$ that contain a term $$t$$. We can represent this with set-builder notation. 

$$|\{d \in D : t \in d\}|$$

To acheive a measure of how much information a term provides, we logarithmically scale the inverse fraction of the ducuments that contain the term. This is our **inverse document frequency**.

$$\text{idf}(t,D) = \log{\frac{|D|}{|\{d \in D : t \in d\}|}}$$

To avoid dividing by zero when a term isn't in the corpus, it is common to increase the denominator by one. 

$$\text{idf}(t,D) = \log{\frac{|D|}{|1 + \{d \in D : t \in d\}|}}$$

Now we can take the product the two statistics to get a composite weight we can apply to each term in each document. 

$$\text{tfidf}(t,d,D) = \text{tf}(t,d) \times \text{idf}(t,D)$$

